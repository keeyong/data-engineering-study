{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1531269362786_-776151570","id":"20180711-003602_1792968724","dateCreated":"2018-07-11T00:36:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:116","text":"%md\n# How do I turn a Python function into a Spark UDF\n\n## Registering a UDF\n\nPySpark UDFs work in a similar way as the pandas .map() and .appy() methods for pandas series and dataframes. If I have a function that can use values from a row in the dataframe as input, then I can map it to the entire dataframe. The only difference is that with PySpark UDFs I have to specify the output data type","dateUpdated":"2018-07-11T00:36:18+0000","dateFinished":"2018-07-11T00:36:18+0000","dateStarted":"2018-07-11T00:36:18+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>How do I turn a Python function into a Spark UDF</h1>\n<h2>Registering a UDF</h2>\n<p>PySpark UDFs work in a similar way as the pandas .map() and .appy() methods for pandas series and dataframes. If I have a function that can use values from a row in the dataframe as input, then I can map it to the entire dataframe. The only difference is that with PySpark UDFs I have to specify the output data type</p>\n"}]}},{"text":"%spark.pyspark\n\nimport pandas as pd\n# need to install pandas in the master node: sudo pip install pandas\n\ndf_pd = pd.DataFrame(\n    data = {\n        'integers': [1, 2, 3],\n        'floats': [-1.0, 0.5, 2.7],\n        'integer_arrays': [[1, 2], [3, 4, 5], [6, 7, 8, 9]]\n    }\n)\ndf = spark.createDataFrame(df_pd)\ndf.printSchema()\ndf.show()\n","user":"anonymous","dateUpdated":"2018-07-11T00:56:52+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1531269378917_1802850956","id":"20180711-003618_1331118440","dateCreated":"2018-07-11T00:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:276","dateFinished":"2018-07-11T00:42:49+0000","dateStarted":"2018-07-11T00:42:42+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- floats: double (nullable = true)\n |-- integer_arrays: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- integers: long (nullable = true)\n\n+------+--------------+--------+\n|floats|integer_arrays|integers|\n+------+--------------+--------+\n|  -1.0|        [1, 2]|       1|\n|   0.5|     [3, 4, 5]|       2|\n|   2.7|  [6, 7, 8, 9]|       3|\n+------+--------------+--------+\n\n"}]}},{"text":"%spark.pyspark\n\ndef square(x):\n    return x**2\n    \nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import udf\n\nsquare_udf_int = udf(lambda z: square(z), IntegerType())\n(\n    df.select(\n        'integers',\n        'floats',\n        square_udf_int('integers').alias('int_squared'),\n        square_udf_int('floats').alias('float_squared')\n    ).show()\n)","user":"anonymous","dateUpdated":"2018-07-11T01:02:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1531269395445_-150503885","id":"20180711-003635_1500410023","dateCreated":"2018-07-11T00:36:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:356","dateFinished":"2018-07-11T01:02:14+0000","dateStarted":"2018-07-11T01:02:08+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+------+-----------+-------------+\n|integers|floats|int_squared|float_squared|\n+--------+------+-----------+-------------+\n|       1|  -1.0|          1|         null|\n|       2|   0.5|          4|         null|\n|       3|   2.7|          9|         null|\n+--------+------+-----------+-------------+\n\n"}]}},{"text":"%spark.pyspark\nfrom pyspark.sql.types import FloatType\n\ndef square_float(x):\n    return float(x**2)\n\nsquare_udf_float = udf(lambda z: square_float(z), FloatType())\n(\n    df.select(\n        'integers',\n        'floats',\n        square_udf_float('integers').alias('int_squared'),\n        square_udf_float('floats').alias('float_squared')\n    ).show()\n)\n","user":"anonymous","dateUpdated":"2018-07-11T01:05:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1531270786344_-1799917487","id":"20180711-005946_303625070","dateCreated":"2018-07-11T00:59:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:485","dateFinished":"2018-07-11T01:05:50+0000","dateStarted":"2018-07-11T01:05:43+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+------+-----------+-------------+\n|integers|floats|int_squared|float_squared|\n+--------+------+-----------+-------------+\n|       1|  -1.0|        1.0|          1.0|\n|       2|   0.5|        4.0|         0.25|\n|       3|   2.7|        9.0|         7.29|\n+--------+------+-----------+-------------+\n\n"}]}},{"text":"%spark.pyspark\nfrom pyspark.sql.types import ArrayType\n\ndef square_list(x):\n    return [float(val)**2 for val in x]\n\nsquare_list_udf = udf(lambda y: square_list(y), ArrayType(FloatType()))\ndf.select('integer_arrays', square_list_udf('integer_arrays')).show()\nprint(spark.sparkContext.uiWebUrl)","user":"anonymous","dateUpdated":"2018-07-11T01:09:10+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1531271114470_-403772642","id":"20180711-010514_1433655761","dateCreated":"2018-07-11T01:05:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:629","dateFinished":"2018-07-11T01:09:10+0000","dateStarted":"2018-07-11T01:09:10+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+------------------------+\n|integer_arrays|<lambda>(integer_arrays)|\n+--------------+------------------------+\n|        [1, 2]|              [1.0, 4.0]|\n|     [3, 4, 5]|       [9.0, 16.0, 25.0]|\n|  [6, 7, 8, 9]|    [36.0, 49.0, 64.0...|\n+--------------+------------------------+\n\nhttp://ip-172-30-1-23.us-west-2.compute.internal:4040\n"}]}},{"text":"%spark.pyspark\ndf_repartitioned = df.repartition(100)\n\n# When a dataframe is repartitioned, I think each executor processes one partition at a time and thus reduce the execution time of the PySpark function to roughly the executio time of Python function times the reciprocal of the number of executors, barring the overhead of initializing a task","user":"anonymous","dateUpdated":"2018-07-11T01:11:30+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1531271279177_-482444784","id":"20180711-010759_536087158","dateCreated":"2018-07-11T01:07:59+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:734"}],"name":"Python UDF","id":"2DHUNV8H9","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}